{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the python library for pruning\n",
    "!pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import losses, activations, models\n",
    "from keras.layers import Dense, InputLayer,BatchNormalization, Dropout, Conv2D, Flatten, Activation\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "       inputs = keras.Input(shape=(28, 28, 1))\n",
    "       x = Conv2D(32, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding=\"same\",\n",
    "                     use_bias=False)(inputs)\n",
    "       x = BatchNormalization()(x)\n",
    "       x = Activation(\"relu\")(x)\n",
    "       x = Conv2D(64, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding=\"same\",\n",
    "                     use_bias=False)(x)\n",
    "       x = BatchNormalization()(x)\n",
    "       x = Activation(\"relu\")(x)\n",
    "       x = Flatten()(x)\n",
    "       x = Dropout(0.5)(x)\n",
    "       outputs = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "       model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "       \n",
    "       opt = tf.keras.optimizers.Adam(0.001)\n",
    "       model.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "       return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 14, 14, 32)        288       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 14, 14, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 64)          18432     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 7, 7, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                31370     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,474\n",
      "Trainable params: 50,282\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_given = get_model()\n",
    "model_given.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_summary(model, x_train, y_train):\n",
    "    # Training the model to get the weights\n",
    "    batch_size = 32\n",
    "    epochs = 1\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "    sparsity_info = []\n",
    "    def layer_weights(model):        \n",
    "        # Get the weights from the layers\n",
    "        for layer in model.layers:\n",
    "            if is_hier_layer(layer):\n",
    "                layer_weights(layer)\n",
    "            else:\n",
    "                weights = layer.get_weights()\n",
    "                if len(weights) > 0:\n",
    "                    total_weights = 0\n",
    "                    non_zero_weights = 0\n",
    "                    for w in weights:\n",
    "                        total_weights += np.prod(w.shape)\n",
    "                        non_zero_weights += np.count_nonzero(w)\n",
    "                    sparsity = 1.0 - (non_zero_weights / total_weights)\n",
    "                    sparsity_info.append(f'sparsity={sparsity:.4f} of layer={layer.name}')\n",
    "        return   \n",
    "    \n",
    "    # Function to check if a layer is model or not\n",
    "    def is_hier_layer(layer):\n",
    "        \"Finds if layer is actually a model instead of a single layer\"\n",
    "        return type(layer) in [models.Sequential, keras.Model]\n",
    "    \n",
    "    layer_weights(model)\n",
    "    return sparsity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 25s 12ms/step - loss: 0.5083 - accuracy: 0.8236 - val_loss: 0.3395 - val_accuracy: 0.8752\n",
      "sparsity=0.0000 of layer=conv2d\n",
      "sparsity=0.0000 of layer=batch_normalization\n",
      "sparsity=0.0000 of layer=conv2d_1\n",
      "sparsity=0.0000 of layer=batch_normalization_1\n",
      "sparsity=0.0000 of layer=dense\n"
     ]
    }
   ],
   "source": [
    "x = weight_summary(model_given,x_train,y_train)\n",
    "for i in x:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_weight_summary(model_given, x_train, y_train):  \n",
    "    prune_sparsity_info = []  # Moved sparsity_info inside the function\n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "    # Compute end step to finish pruning after 2 epochs.\n",
    "    batch_size = 128\n",
    "    epochs = 2\n",
    "    validation_split = 0.1 # 10% of training set will be used for validation set.\n",
    "    num_images = x_train.shape[0] * (1 - validation_split)\n",
    "    end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "    # Define model for pruning.\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.40,\n",
    "                                                                final_sparsity=0.60,\n",
    "                                                                begin_step=0,\n",
    "                                                                end_step=end_step)\n",
    "    }\n",
    "\n",
    "    model_for_pruning = prune_low_magnitude(model_given, **pruning_params)\n",
    "\n",
    "    # `prune_low_magnitude` requires a recompile.\n",
    "    model_for_pruning.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    model_for_pruning.summary()\n",
    "    \n",
    "    \n",
    "    # Training the model to get the weights\n",
    "    logdir = tempfile.mkdtemp()\n",
    "    callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "    ]\n",
    "    \n",
    "    batch_size = 32\n",
    "    epochs = 1\n",
    "    model_for_pruning.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,callbacks=callbacks)  \n",
    "    \n",
    "    def prune_layer_weights(model_pruned):\n",
    "        \n",
    "        for layer in model_pruned.layers:\n",
    "            if is_hier_layer(layer):\n",
    "                prune_layer_weights(layer)\n",
    "            else:                \n",
    "                # Get the weights from the layers\n",
    "                weights = layer.get_weights()\n",
    "                if len(weights) > 0:\n",
    "                    total_weights = 0\n",
    "                    non_zero_weights = 0\n",
    "                    for w in weights:\n",
    "                        total_weights += np.prod(w.shape)\n",
    "                        non_zero_weights += np.count_nonzero(w)\n",
    "                    sparsity = 1.0 - (non_zero_weights / total_weights)\n",
    "                    prune_sparsity_info.append(f'sparsity={sparsity:.4f} of layer={layer.name}')\n",
    "\n",
    "    # Function to check if a layer is model or not\n",
    "    def is_hier_layer(layer):\n",
    "        \"Finds if layer is actually a model instead of a single layer\"\n",
    "        return type(layer) in [models.Sequential, keras.Model]\n",
    "\n",
    "    prune_layer_weights(model_for_pruning)\n",
    "    return prune_sparsity_info,model_for_pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nithya Raj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d   (None, 14, 14, 32)       578       \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 14, 14, 32)       129       \n",
      " ormalization (PruneLowMagni                                     \n",
      " tude)                                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_activat  (None, 14, 14, 32)       1         \n",
      " ion (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 7, 7, 64)         36866     \n",
      " 1 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 7, 7, 64)         257       \n",
      " ormalization_1 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_activat  (None, 7, 7, 64)         1         \n",
      " ion_1 (PruneLowMagnitude)                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_flatten  (None, 3136)             1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_dropout  (None, 3136)             1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_dense (  (None, 10)               62732     \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 100,566\n",
      "Trainable params: 50,282\n",
      "Non-trainable params: 50,284\n",
      "_________________________________________________________________\n",
      "   5/1875 [..............................] - ETA: 27s - loss: 0.4157 - accuracy: 0.8438    WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0139s vs `on_train_batch_end` time: 0.0152s). Check your callbacks.\n",
      "1875/1875 [==============================] - 24s 11ms/step - loss: 0.3561 - accuracy: 0.8721\n",
      "['sparsity=0.6007 of layer=prune_low_magnitude_conv2d', 'sparsity=0.0000 of layer=prune_low_magnitude_batch_normalization', 'sparsity=0.6000 of layer=prune_low_magnitude_conv2d_1', 'sparsity=0.0000 of layer=prune_low_magnitude_batch_normalization_1', 'sparsity=0.5998 of layer=prune_low_magnitude_dense']\n"
     ]
    }
   ],
   "source": [
    "pruned_data, model_for_pruning = prune_weight_summary(model_given,x_train,y_train)\n",
    "print(pruned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non-sparsity_probability(model_for_pruning,pruned_data):\n",
    "    # Parse the sparsity values and layer names\n",
    "    probability_pruned_values = {}\n",
    "    prefix_name = model_for_pruning.name + \"_\"\n",
    "    # non_zero_sparsity_values = []\n",
    "    for line in pruned_data:\n",
    "        parts = line.split()\n",
    "        sparsity = float(parts[0].split('=')[1])\n",
    "        layer_intended = parts[-1].split('=')[1]\n",
    "        if sparsity != 1.0:\n",
    "            for layer in model_for_pruning.layers:\n",
    "                if layer.name == layer_intended:\n",
    "                    weights = layer.get_weights()\n",
    "                    if len(weights) > 0:\n",
    "                        non_zero_weights = 0\n",
    "                        total_weights = 0\n",
    "                        for w in weights:\n",
    "                            total_weights += np.prod(w.shape)\n",
    "                            non_zero_weights += np.count_nonzero(w)\n",
    "                        probability = non_zero_weights/total_weights\n",
    "                        probability_pruned_values[prefix_name + layer.name] = probability\n",
    "    return probability_pruned_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_prune_low_magnitude_conv2d': 0.3993055555555556,\n",
       " 'model_prune_low_magnitude_batch_normalization': 1.0,\n",
       " 'model_prune_low_magnitude_conv2d_1': 0.4000108506944444,\n",
       " 'model_prune_low_magnitude_batch_normalization_1': 1.0,\n",
       " 'model_prune_low_magnitude_dense': 0.4002231431303793}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non-sparsity_probability(model_for_pruning,pruned_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "253dfbcfd973045a846042842c7d4e4bbeaf4f6a02166bd713718c08e40fa8ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
