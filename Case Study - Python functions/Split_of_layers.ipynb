{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install netron"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVQrY2PfIuuE",
        "outputId": "d77e9fac-a87f-4ac3-8560-689a7b49513c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting netron\n",
            "  Downloading netron-7.1.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: netron\n",
            "Successfully installed netron-7.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "import netron\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "ndRfQQRc_6Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_zo_conv(ll, ZO_cut):\n",
        "    \"Split a convolution layer ll into pieces in Z as given by ZO_cut. The filters of layer equal sum(ZO_cut)\"\n",
        "\n",
        "    # Get  configuration\n",
        "    cfg = ll.get_config()\n",
        "\n",
        "    # Check we are dealing with simple case\n",
        "    if not( cfg['dilation_rate']==(1,1) or cfg['data_format']=='channels_last' or cfg['activation']=='linear'):\n",
        "            raise Exception(\"Sorry, this case for convolution not implemented yet\")\n",
        "\n",
        "    # Extract weights and bias\n",
        "    if cfg['use_bias']:\n",
        "        w, bias = ll.get_weights()\n",
        "    else:\n",
        "        w,      = ll.get_weights()\n",
        "\n",
        "    name_t = cfg['name'] + \"_\"\n",
        "    cfg_t = cfg.copy()\n",
        "    del cfg_t['filters']\n",
        "    del cfg_t['name']\n",
        "\n",
        "    # Create the new conv layers\n",
        "    lls = [layers.Conv2D(filters=flt, name=name_t + str(it), **cfg_t) for it, flt in enumerate(ZO_cut)]\n",
        "\n",
        "    # Create a tensor Input\n",
        "    x = layers.Input(shape=ll.input_shape[1:]) # Note that BatchSize=None is removed..\n",
        "\n",
        "    # Connect tensor input to layers\n",
        "    lls_tf = [lli(x) for lli in lls]\n",
        "\n",
        "    # Add weights\n",
        "    idx=0\n",
        "    for lli, flt in zip(lls, ZO_cut):\n",
        "        if cfg['use_bias']:\n",
        "            lli.set_weights([w[:,:,:, idx:idx+flt].copy(), bias.copy()])\n",
        "        else:\n",
        "            lli.set_weights([w[:,:,:, idx:idx+flt].copy()])\n",
        "        idx += flt\n",
        "\n",
        "    # Concatenate outputs\n",
        "    y = layers.Concatenate()(lls_tf)  # Concatenate in axis=-1\n",
        "\n",
        "    m = models.Model(x, y)\n",
        "\n",
        "    return m"
      ],
      "metadata": {
        "id": "gRzGzZBS_9Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_yo_conv(ll, YO_cut):\n",
        "    \"\"\"It splits a convolution layer =ll= into pieces in Y direction\n",
        "    as given by the list YO_cut. The number of lines of layer equal sum(YO_cut)\n",
        "\n",
        "    It assumes no dilation, standard data_format, and\n",
        "    \"\"\"\n",
        "\n",
        "    # Get  configuration\n",
        "    cfg = ll.get_config()\n",
        "\n",
        "    # Check we are dealing with simple case\n",
        "    if not( cfg['dilation_rate']==(1,1) or cfg['data_format']=='channels_last' or cfg['activation']=='linear'):\n",
        "            raise Exception(\"Sorry, this case for convolution not implemented yet\")\n",
        "\n",
        "    # Limits of YO\n",
        "    Ky = cfg['kernel_size'][1]  # Kernel size in Y direction\n",
        "    Sy = cfg['strides'][1]      # Stride in Y direction\n",
        "    lim = np.cumsum([0] + YO_cut)\n",
        "    # Regions in input (accounting for stride and kernel size)\n",
        "    reg = list([Sy*r, Sy*(s-1) + Ky-1 ] for r, s in   zip(lim[:-1], lim[1:]))\n",
        "\n",
        "    # Create the new conv layers with same params as initial one, but name\n",
        "    name_t = cfg['name'] + \"_\"\n",
        "    cfg_t = cfg.copy()\n",
        "    del cfg_t['name']\n",
        "    lls = [layers.Conv2D(name=name_t + str(it), **cfg_t) for it,_ in enumerate(YO_cut) ]\n",
        "\n",
        "    # Create a tensor Input of same size as initial\n",
        "    x = layers.Input(shape=ll.input_shape[1:]) # Note that BatchSize=None is removed..\n",
        "\n",
        "    # Connect tensor input to layers according to region limits\n",
        "    lls_tf = [lli(x[:,:,r:s+1,:]) for (r, s), lli in (zip(reg, lls))]  # Note +1 because of python limits\n",
        "\n",
        "    # Add weights\n",
        "    for lli in (lls):\n",
        "        lli.set_weights(ll.get_weights())  # Note it is a link, not a copy!\n",
        "\n",
        "    # Concatenate outputs\n",
        "    y = layers.Concatenate(axis=-2)(lls_tf)  # Concatenate in axis=-2 (ie., Y)\n",
        "\n",
        "    m = models.Model(x, y)\n",
        "\n",
        "    return m\n"
      ],
      "metadata": {
        "id": "QISSxAaG_11G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float')/255\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float')/255\n",
        "\n",
        "# Create Model\n",
        "model = models.Sequential(\n",
        "    [\n",
        "        layers.Conv2D(32, kernel_size=(3, 3),\n",
        "                      input_shape=(28, 28, 1),\n",
        "                      strides=(2,2), padding=\"same\",\n",
        "                      use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation(activation=\"relu\"),\n",
        "        #layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3),\n",
        "                      strides=(2,2), padding=\"same\",\n",
        "                      use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation(activation=\"relu\"),\n",
        "        #layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Plotting the model\n",
        "# Using netron\n",
        "# model_name = \"Original model.h5\"\n",
        "# model.save(model_name)\n",
        "# netron.start(model_name,8088)\n",
        "\n",
        "# Using utils\n",
        "plot_model(model)\n",
        "\n",
        "# Train\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x=x_train, y=y_train, epochs=2, batch_size=128, validation_split=0.1 )\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "\n",
        "# Now create a new model removing a conv layer by the one we have generated\n",
        "\n",
        "ll = model.layers[3]\n",
        "\n",
        "ZO_cut = [32, 16, 16]\n",
        "\n",
        "aux = split_zo_conv(ll, ZO_cut)\n",
        "\n",
        "modelz_lst = model.layers\n",
        "modelz_lst[3] = aux\n",
        "\n",
        "modelz = models.Sequential(modelz_lst)\n",
        "\n",
        "\n",
        "modelz.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Plotting the modified model\n",
        "# Using netron\n",
        "model_name = \"Modified model.h5\"\n",
        "modelz.save(model_name)\n",
        "netron.start(model_name,8089)\n",
        "plot_model(modelz)\n",
        "score2 = modelz.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(score)\n",
        "print(score2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO5gKMxyEDCl",
        "outputId": "6502a815-bd4b-4cc0-cd2e-28ffd81faea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "422/422 [==============================] - 25s 56ms/step - loss: 0.5454 - accuracy: 0.8094 - val_loss: 0.7105 - val_accuracy: 0.7512\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 23s 55ms/step - loss: 0.3762 - accuracy: 0.8654 - val_loss: 0.3206 - val_accuracy: 0.8815\n",
            "Serving 'Modified model.h5' at http://localhost:8089\n",
            "[0.3383619487285614, 0.8780999779701233]\n",
            "[0.3383619487285614, 0.0997999981045723]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float')/255\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float')/255\n",
        "\n",
        "# Create Model\n",
        "model = models.Sequential(\n",
        "    [\n",
        "        layers.Conv2D(32, kernel_size=(3, 3),\n",
        "                      input_shape=(28, 28, 1),\n",
        "                      strides=(2,2), padding=\"same\",\n",
        "                      use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation(activation=\"relu\"),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3),\n",
        "                      strides=(2,2), padding=\"valid\",\n",
        "                      use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation(activation=\"relu\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x=x_train, y=y_train, epochs=1, batch_size=128, validation_split=0.1 )\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "\n",
        "# Now create a new model removing a conv layer by the one we have generated\n",
        "\n",
        "ll = model.layers[3]\n",
        "print(ll.output_shape)\n",
        "YO_cut = [3, 3]\n",
        "\n",
        "aux = split_yo_conv(ll, YO_cut)\n",
        "\n",
        "model2_lst = model.layers\n",
        "model2_lst[3] = aux\n",
        "\n",
        "model2 = models.Sequential(model2_lst)\n",
        "model2.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(score)\n",
        "print(score2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O83aHYO5_WmQ",
        "outputId": "9bf0f356-ec4f-42de-cb7c-e4d6e69cc7ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "422/422 [==============================] - 21s 47ms/step - loss: 0.5507 - accuracy: 0.8039 - val_loss: 0.6734 - val_accuracy: 0.7952\n",
            "(None, 6, 6, 64)\n",
            "[0.6847608685493469, 0.791700005531311]\n",
            "[0.6847608685493469, 0.791700005531311]\n"
          ]
        }
      ]
    }
  ]
}